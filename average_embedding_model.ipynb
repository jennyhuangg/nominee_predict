{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jhuang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jhuang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "get_ipython().magic('matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AJGoldberg', 'AMKennedy', 'AScalia', 'BRWhite', 'DHSouter', 'EKagan', 'JGRoberts', 'JPStevens', 'LFPowell', 'NMGorsuch', 'RBGinsburg', 'SAAlito', 'SDOConnor', 'SGBreyer', 'SSotomayor', 'TMarshall', 'WHRehnquist']\n"
     ]
    }
   ],
   "source": [
    "# Get file names\n",
    "in_dir = \"nlp_approach/data/transcripts/utterances\"\n",
    "files = os.listdir(in_dir)\n",
    "files.sort()\n",
    "new_files = []\n",
    "justicenames = []\n",
    "for f in files:\n",
    "    if f.endswith(\".txt\"):\n",
    "        new_files.append(os.path.join(in_dir,f))\n",
    "        justicenames.append(f[:-4])\n",
    "\n",
    "print(justicenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 22:06:14,164 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin.gz\n",
      "2019-05-10 22:06:14,166 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 22:09:21,423 : INFO : loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "w = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 22:10:53,272 : INFO : saving Word2VecKeyedVectors object under pretrained_word2vec_model, separately None\n",
      "2019-05-10 22:10:53,273 : INFO : storing np array 'vectors' to pretrained_word2vec_model.vectors.npy\n",
      "2019-05-10 22:11:33,382 : INFO : not storing attribute vectors_norm\n",
      "2019-05-10 22:11:33,421 : WARNING : this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "w.save(\"pretrained_word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def get_corpus(files):\n",
    "    corpus = []\n",
    "    for f in files:\n",
    "        data = f.read().replace('\\n', '')\n",
    "        data = preprocess(data)\n",
    "        doc = gensim.utils.simple_preprocess(data, deacc=True)\n",
    "        corpus.append(doc)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(word2vec_model, words):\n",
    "    # remove out-of-vocabulary words\n",
    "    words = [word for word in words if word in word2vec_model.vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(word2vec_model[words], axis=0)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_corpus(new_files)\n",
    "for doc in corpus:\n",
    "    vec = get_mean_vector(w, doc)\n",
    "    if len(vec) > 0:\n",
    "      # do somthing with the vector ${vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_files):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "\n",
    "    lines = []\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, 'rb') as f:\n",
    "            logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "            for i, line in enumerate(f):\n",
    "                # do some pre-processing and return list of words for each review\n",
    "                # text\n",
    "                lines.append(gensim.utils.simple_preprocess(line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 21:47:09,891 : INFO : reading file nlp_approach/data/transcripts/utterances/AJGoldberg.txt...this may take a while\n",
      "2019-05-10 21:47:09,909 : INFO : reading file nlp_approach/data/transcripts/utterances/AMKennedy.txt...this may take a while\n",
      "2019-05-10 21:47:09,992 : INFO : reading file nlp_approach/data/transcripts/utterances/AScalia.txt...this may take a while\n",
      "2019-05-10 21:47:10,043 : INFO : reading file nlp_approach/data/transcripts/utterances/BRWhite.txt...this may take a while\n",
      "2019-05-10 21:47:10,049 : INFO : reading file nlp_approach/data/transcripts/utterances/DHSouter.txt...this may take a while\n",
      "2019-05-10 21:47:10,183 : INFO : reading file nlp_approach/data/transcripts/utterances/EKagan.txt...this may take a while\n",
      "2019-05-10 21:47:10,263 : INFO : reading file nlp_approach/data/transcripts/utterances/JGRoberts.txt...this may take a while\n",
      "2019-05-10 21:47:10,367 : INFO : reading file nlp_approach/data/transcripts/utterances/JPStevens.txt...this may take a while\n",
      "2019-05-10 21:47:10,393 : INFO : reading file nlp_approach/data/transcripts/utterances/LFPowell.txt...this may take a while\n",
      "2019-05-10 21:47:10,424 : INFO : reading file nlp_approach/data/transcripts/utterances/NMGorsuch.txt...this may take a while\n",
      "2019-05-10 21:47:10,542 : INFO : reading file nlp_approach/data/transcripts/utterances/RBGinsburg.txt...this may take a while\n",
      "2019-05-10 21:47:10,590 : INFO : reading file nlp_approach/data/transcripts/utterances/SAAlito.txt...this may take a while\n",
      "2019-05-10 21:47:10,699 : INFO : reading file nlp_approach/data/transcripts/utterances/SDOConnor.txt...this may take a while\n",
      "2019-05-10 21:47:10,761 : INFO : reading file nlp_approach/data/transcripts/utterances/SGBreyer.txt...this may take a while\n",
      "2019-05-10 21:47:10,842 : INFO : reading file nlp_approach/data/transcripts/utterances/SSotomayor.txt...this may take a while\n",
      "2019-05-10 21:47:10,920 : INFO : reading file nlp_approach/data/transcripts/utterances/TMarshall.txt...this may take a while\n",
      "2019-05-10 21:47:10,948 : INFO : reading file nlp_approach/data/transcripts/utterances/WHRehnquist.txt...this may take a while\n"
     ]
    }
   ],
   "source": [
    "documents = read_input(new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 21:47:19,612 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-05-10 21:47:19,613 : INFO : collecting all words and their counts\n",
      "2019-05-10 21:47:19,614 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-10 21:47:19,712 : INFO : PROGRESS: at sentence #10000, processed 338550 words, keeping 11442 word types\n",
      "2019-05-10 21:47:19,771 : INFO : collected 14927 word types from a corpus of 618049 raw words and 18459 sentences\n",
      "2019-05-10 21:47:19,772 : INFO : Loading a fresh vocabulary\n",
      "2019-05-10 21:47:19,803 : INFO : effective_min_count=2 retains 8942 unique words (59% of original 14927, drops 5985)\n",
      "2019-05-10 21:47:19,804 : INFO : effective_min_count=2 leaves 612064 word corpus (99% of original 618049, drops 5985)\n",
      "2019-05-10 21:47:19,842 : INFO : deleting the raw counts dictionary of 14927 items\n",
      "2019-05-10 21:47:19,843 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-05-10 21:47:19,844 : INFO : downsampling leaves estimated 423299 word corpus (69.2% of prior 612064)\n",
      "2019-05-10 21:47:19,883 : INFO : estimated required memory for 8942 words and 150 dimensions: 15201400 bytes\n",
      "2019-05-10 21:47:19,884 : INFO : resetting layer weights\n",
      "2019-05-10 21:47:20,011 : INFO : training model with 10 workers on 8942 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-05-10 21:47:20,441 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-10 21:47:20,441 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-10 21:47:20,442 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-10 21:47:20,444 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-10 21:47:20,518 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-10 21:47:20,549 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-10 21:47:20,557 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 21:47:20,580 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 21:47:20,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 21:47:20,583 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 21:47:20,584 : INFO : EPOCH - 1 : training on 618049 raw words (423358 effective words) took 0.6s, 748983 effective words/s\n",
      "2019-05-10 21:47:21,122 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-10 21:47:21,124 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-10 21:47:21,131 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-10 21:47:21,133 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-10 21:47:21,144 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-10 21:47:21,145 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-10 21:47:21,146 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 21:47:21,147 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 21:47:21,148 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 21:47:21,158 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 21:47:21,159 : INFO : EPOCH - 2 : training on 618049 raw words (422847 effective words) took 0.5s, 773666 effective words/s\n",
      "2019-05-10 21:47:21,578 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-10 21:47:21,593 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-10 21:47:21,598 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-10 21:47:21,605 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-10 21:47:21,606 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-10 21:47:21,609 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-10 21:47:21,610 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 21:47:21,613 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 21:47:21,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 21:47:21,637 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 21:47:21,638 : INFO : EPOCH - 3 : training on 618049 raw words (423172 effective words) took 0.5s, 900347 effective words/s\n",
      "2019-05-10 21:47:22,194 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-10 21:47:22,196 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-10 21:47:22,198 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-10 21:47:22,199 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-10 21:47:22,206 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-10 21:47:22,213 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-10 21:47:22,214 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 21:47:22,216 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 21:47:22,221 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 21:47:22,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 21:47:22,237 : INFO : EPOCH - 4 : training on 618049 raw words (422981 effective words) took 0.6s, 722728 effective words/s\n",
      "2019-05-10 21:47:22,726 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-10 21:47:22,736 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-10 21:47:22,736 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-10 21:47:22,739 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-10 21:47:22,741 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-10 21:47:22,748 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-10 21:47:22,749 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 21:47:22,754 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 21:47:22,757 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 21:47:22,765 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 21:47:22,765 : INFO : EPOCH - 5 : training on 618049 raw words (423079 effective words) took 0.5s, 826873 effective words/s\n",
      "2019-05-10 21:47:22,766 : INFO : training on a 3090245 raw words (2115437 effective words) took 2.8s, 768142 effective words/s\n"
     ]
    }
   ],
   "source": [
    " model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        size=150,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 21:48:15,701 : INFO : saving Word2Vec object under word2vec_model, separately None\n",
      "2019-05-10 21:48:15,702 : INFO : not storing attribute vectors_norm\n",
      "2019-05-10 21:48:15,704 : INFO : not storing attribute cum_table\n",
      "2019-05-10 21:48:15,705 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 21:48:15,889 : INFO : saved word2vec_model\n"
     ]
    }
   ],
   "source": [
    "model.save(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 21:48:39,921 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot compute similarity with no input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-1741c8b2a914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute similarity with no input"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(f):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-eb0ec1a551e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nlp_approach/data/transcripts/utterances/AJGoldberg.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-1617aa7c7e78>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "vocabulary = read_data(\"nlp_approach/data/transcripts/utterances/AJGoldberg.txt\")\n",
    "print(vocabulary[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_model(embedding_size, input_file=\"data/transcripts/utterances\"):\n",
    "    \"\"\"\n",
    "    Create the word2vec model based on the given embedding size and the corpus file.\n",
    "    :param embedding_size: The embedding size\n",
    "    :param input_file: The corpus file\n",
    "    \"\"\"\n",
    "    word2vec_file = 'word2vec_' + str(embedding_size) + '.model'\n",
    "\n",
    "    if os.path.isfile(word2vec_file):\n",
    "        logging.info('? The word2vec model you want create already exists!')\n",
    "    else:\n",
    "        sentences = word2vec.LineSentence(input_file)\n",
    "        # sg=0 means use CBOW model(default); sg=1 means use skip-gram model.\n",
    "        model = gensim.models.Word2Vec(sentences, size=embedding_size, min_count=0,\n",
    "                                       sg=0, workers=multiprocessing.cpu_count())\n",
    "        model.save(word2vec_file) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=100\n",
    "input_file=\"nlp_approach/data/transcripts/utterances/test.txt\"\n",
    "word2vec_file = 'word2vec_' + str(embedding_size) + '.model'\n",
    "\n",
    "if os.path.isfile(word2vec_file):\n",
    "    logging.info('? The word2vec model you want create already exists!')\n",
    "else:\n",
    "    sentences = word2vec.LineSentence(input_file)\n",
    "    # sg=0 means use CBOW model(default); sg=1 means use skip-gram model.\n",
    "    model = gensim.models.Word2Vec(sentences, size=embedding_size, min_count=0,\n",
    "                                   sg=0, workers=multiprocessing.cpu_count())\n",
    "    model.save(word2vec_file) \n",
    "    model.train(documents, total_examples=len(documents), epochs=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    new_files,\n",
    "    size=150,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=10)\n",
    "model.train(documents, total_examples=len(documents), epochs=10)\n",
    "\n",
    "### Tokenize, remove stopwords\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "corpus = [preprocess(text) for text in texts]\n",
    "\n",
    "# ### Remove OOV words and documents with no words in model dictionar\n",
    "\n",
    "\n",
    "x =[]\n",
    "for doc in corpus: #look up each doc in model\n",
    "    x.append(document_vector(model, doc))\n",
    "\n",
    "\n",
    "X = np.array(x) #list to array\n",
    "\n",
    "np.save('documents_vectors.npy', X)  #np.savetxt('documents_vectors.txt', X)\n",
    "np.save('labels.npy', y)             #np.savetxt('labels.txt', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot 2 PCA components\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "plt.figure(1, figsize=(30, 20),)\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1],s=100, c=y, alpha=0.2)\n",
    "\n",
    "\n",
    "# ### Plot t-SNE\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "X_tsne = TSNE(n_components=2, verbose=2).fit_transform(X)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "plt.figure(1, figsize=(30, 20),)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1],s=100, c=y, alpha=0.2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "© 2019 GitHub, Inc.\n",
    "Terms\n",
    "Privacy\n",
    "Security\n",
    "Status\n",
    "Help\n",
    "Contact GitHub\n",
    "Pricing\n",
    "API\n",
    "Training\n",
    "Blog\n",
    "About"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
